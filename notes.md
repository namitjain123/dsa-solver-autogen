ProblemSolverAgent (LLM)
        â†“
CodeExecutorAgent (Tool Agent)
        â†“
DockerCommandLineCodeExecutor (Runtime Engine)
        â†“
Docker Container (Sandbox)
        â†“
Python Execution
        â†“
Output Returned

Your system now looks like this:

User
  â†“
ProblemSolverAgent (LLM)
  â†“ sends python code
CodeExecutorAgent (Tool Agent)
  â†“
Docker Executor
  â†“
Execution Output
  â†“
Back to ProblemSolverAgent


This is a separation of intelligence and execution.


A Tool Agent is an agent that:

Does not think.
Does not call an LLM.
Does not generate language.
It performs deterministic actions.

It is basically:

LLM Brain  â†’  Tool Agent  â†’  Real-world action

## autogen architecture
In AutoGen Architecture

There are generally two categories:

1ï¸âƒ£ LLM Agents (Cognitive)

Use model_client

Generate text

Plan, reason, decide

Example: AssistantAgent

2ï¸âƒ£ Tool Agents (Execution / Action)

No model_client

Execute something

Perform structured tasks

Example: CodeExecutorAgent



ğŸ” So The Flow Is:
Step 1 â€” LLM Agent generates code
```python
print("Hello")
```

Step 2 â€” Tool Agent detects code block

It extracts:

print("Hello")

Step 3 â€” Tool Agent sends code to Docker executor

Docker runs:

python temp_script.py

Step 4 â€” Docker returns:
Hello

Step 5 â€” Tool Agent wraps result into message

It sends back:

Execution Result:
Hello


Still no LLM involved.


LLM â†’ writes instructions
Tool Agent â†’ forwards instructions
Docker â†’ executes instructions
Tool Agent â†’ forwards output
LLM â†’ interprets output
